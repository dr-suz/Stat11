---
title: "Stat 11 Week 12"
subtitle: "Comparing Groups" 
author: "Prof. Suzy Thornton"
date: "Spring 2023"
lang: "en-US"
output:
  html_document:
    df_print: paged
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsfonts}
- \usepackage{xcolor}
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```



# 1. Finishing up last week's worksheet 

Today, we will cover the [solutions for Part 2 of last week's worksheet](https://profsuzy.github.io/Stat11/wk11-wksheet-solns.html). 



# 2. Comparing Groups: Difference between proportions  

Under rather general conditions, the sampling distribution of the difference between two independent proportions is a Normal distribution with expectation,
$$E(\hat{p}_1 - \hat{p}_2) = p_1 - p_2$$ 
and variance 
$$Var(\hat{p}_1 - \hat{p}_2) = \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}.$$
The estimate for the sampling standard deviation of $\hat{p}_1 - \hat{p}_2$ is the standard error: 
$$SE(\hat{p}_1 - \hat{p}_2) = \sqrt{ \frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}.$$

## Assumptions and conditions 

* Independence assumption 

  * Each sample consists of independent observations
  
  * The two samples are independent from one another (unpaired)

* Randomization condition 

* Success/failure condition - both samples must contain enough failures and successes 


## A two-proportion z-interval 

$$(\hat{p}_1 - \hat{p}_2) \pm \left[z^*_{a} \times SE(\hat{p}_1 - \hat{p}_2)\right],$$
where, as before, $z_a^*$ is the lower (or upper) $\left(\frac{1-a}{2}\right)^{th}$ quantile of a $N(0,1)$ distribution. 

## A two-sample z-test 

$$H_0: p_1 - p_2 = 0$$ 

$$T.S. = \frac{(\hat{p}_1 - \hat{p}_2) - 0}{SE\left(\hat{p}_1 - \hat{p}_2 \right)} \stackrel{H_0}{\sim} N(0,1)$$



# 3. Comparing Groups: Difference between two means

Under rather general conditions, the sampling distribution of the difference between two independent means is a Normal distribution with expectation,
$$E(\bar{x}_1 - \bar{x}_2) = \mu_1 - \mu_2$$ 
and variance 
$$Var(\bar{x}_1 - \bar{x}_2) = \frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}.$$
The estimate for the sampling standard deviation of $\bar{x}_1 - \bar{x}_2$ is the standard error: 
$$SE(\bar{x}_1 - \bar{x}_2) = \sqrt{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}},$$
where $s_1^2 = \frac{1}{n_1-1}\sum_{i=1}^{n_1}(x_{1,i} - \bar{x}_1)^2$ and $s_2^2 = \frac{1}{n_2-1}\sum_{i=1}^{n_2}(x_{2,i} - \bar{x}_2)^2$.


## Assumptions and conditions 

* Independence assumption 

  * Each sample consists of independent observations
  
  * The two samples are independent from one another (unpaired)

* Randomization condition 

* Nearly Normal Condition or both sample sizes are large enough for the CLT to approximate the sampling distribution of $\bar{x}_1$ and $\bar{x}_2$. 


## Two-sample independent t-interval 

$$(\bar{x}_1 - \bar{x}_2) \pm \left[t^*_a \times SE(\bar{x}_1 - \bar{x}_2)  \right],$$
where $t^*_a$ is again, the $\left(\frac{1-a}{2}\right)^{th}$ lower quantile of a Student's t-distribution, but the degrees of freedom are difficulty to calculate, so we typically let computers do this part. 

## Two-sample independent t-test 

$$H_0: \mu_1 - \mu_2 = \Delta_0$$ 

$$T.S. = \frac{(\bar{x}_1 - \bar{x}_2) - \Delta_0}{SE(\bar{x}_1 - \bar{x}_2)} \stackrel{H_0}{\sim} \text{Student's t dist with some (complicated) degrees of freedom}$$

## Pooling 

In the special case where we have data from a randomized experiment, we generally make the assumption that each treatment group has the same population variance. (Random assignment is what makes this assumption justifiable.)

In this special case, we use a pooled standard error for $(\bar{x}_1 - \bar{x}_2)$: 
$$SE_{pool}(\bar{x}_1 - \bar{x}_2) = s_{pool}\sqrt{\frac{1}{n_1} + \frac{1}{n_2}},$$
where $s_{pool} = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{(n_1-1) + (n_2-2)}}.$

# 4. Looking ahead 

Next class, we will discuss how to conduct inference for the difference of paired sample means. That is, we will consider confidence intervals and hypothesis tests for $(\mu_1 - \mu_2)$ when there is a strong relationship between the two populations sampled. 

Spoiler alert: This special case actually just simplifies to a one-sample t-test for a mean! 



