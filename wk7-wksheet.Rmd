---
title: "In-Class Worksheet"
author: "STAT011 with Prof Suzy"
date: "Week 7: Experiments with Random Variables"
output:
  html_document: default
---
<br>
**Name:** _____________________________________

<br>
**Instructions:** There are X parts to this worksheet. First, work on the X questions. Then, when instructed by your professor, get into groups to discuss your answers to Part X and move on to complete Part X. Before getting started however, take a moment and reflect on ways in which you can show your assigned group members respect. You may also view this initial [list of examples](https://docs.google.com/document/d/1emP8SiS2IO-_blKdVmVp-CJTr0NLlKG4vzMnphH-P7E/edit?usp=sharing) that we will add to over the semester.

**Briefly,** in the space below, specify one way in which you will work to show your group members respect during today's lesson:
<br>
<br>
<br>
<br>

```{r setup, echo=FALSE, include=FALSE}
require(knitr)
library(tidyverse)
knitr::opts_chunk$set(message = FALSE) # include this if you don't want markdown to knit messages
knitr::opts_chunk$set(warning = FALSE) # include this if you don't want markdown to knit warnings
```

[bring coins to class and have students flip them (at least 35 coins or one large coin to pass around)]

Class tosses same coin $36$ times.


# 1.  What are the mean and standard deviation of the number of heads? How many standard deviations above (or below) the mean is the number of heads observed (via actual coin tossing)?


<br>
<br>
<br>
<br>



# 2.  What would be the observed number of heads if we observed an amount that was two standard deviations above the mean? If instead we tossed the coin $100$ times, would this same number of heads still be unusual? If not, how many more heads would we need to see to get to an amount that is two standard deviations above the mean? Explain how these results refute the “Law of Averages” but confirm the Law of Large Numbers

<br>
<br>
<br>
<br>



***

normal approximation to the binomial model:
The CLT tells us that, if our sample size is large and our data is drawn independently from the population, then the sampling distribution of the sample proportion is a Normal distribution. More specifically, this will follow a Normal distribution with mean $\mu = p$ and variance $\sigma^2 = \frac{p(1-p)}{30}$.

We're not going to generate a sample of $100$ flips of this coin in reality but we can still calculate an estimate for $p$ with the formula:
$$\hat{p} = \frac{1}{n} \sum_{i=1}^{n}x_i.$$

# 3. What can you say about the value of $\hat{p}$ even before you even see the data? Draw a picture of the density curve for the sampling distribution of the sample proportion. Label the mean and one standard deviation above and below the mean on the curve.


<br>
<br>
<br>
<br>
 


# 4. After $n=100$ flips of the coin, suppose we observe $41$ heads and $59$ tails. What is the value of $\hat{p}$ and how far away from the expected value (or mean) of the sampling distribution of the sample proportions is this? 

<br>
<br>
<br>
<br>


# 5.  What is the probability that, repeating this experiment again and again produces the same value of $\hat{p}$ or something larger? Draw a picture shading the region corresponding to this probability and use the Z-table below to approximate its value.


<br>
<br>
<br>
<br>



![](./images/wk7-ztable.png)




*** 


TO DELETE

Ask class: each coin represents a random variable. specify possibilities and probabilities. 

then conduct simulation exercise to have class demonstrate that RVs can't just be added together or multiplied

One Group represent RV X1, indep of second Group representing RV X2. Each flip their own "group" coin (say 15 times each). identify the probability model and expected heads. Calculate separate sample means, then add together, $\bar{x}_1 + \bar{x}_2$

Then add together and flip coin or use simulator again (simplicity benefit flip coin) and count expected wins, $\bar{y}$
and laws of expectation and variance and covariance 

[can do similar thing with standard deviations. be careful about theory world $E(Y) = E(X_1+X+2) = E(X_1) +E(X_2)]$ vs observable real world with samples, particular instances of RVs, not the actual RVs themselves 

```{r}
#set.seed(101)
X1 <- rbinom(n=1, size = 14, p=0.5)
X2 <- rbinom(n=1, size = 14, p=0.5)
mean(X1) + mean(X2)


Y <- rbinom(n=1, size = 14, p=0.5) + rbinom(n=1, size = 14, p=0.5)
mean(Y)
```




